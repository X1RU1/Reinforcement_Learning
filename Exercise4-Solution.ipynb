{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2LqxoaUO-vR"
      },
      "source": [
        "# Exercise 4: Temporal Difference Learning\n",
        "Giorgia Ramponi, Kristijan Bundaleski\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc98cRv8Y14j"
      },
      "source": [
        "Grid World : A simple, discrete environment where an agent navigates a\n",
        "grid (e.g., 5x5) to achieve goals. The agent learns to make decisions based\n",
        "on states, actions, and rewards."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY2-dQCBObgT"
      },
      "source": [
        "We have an environment that is a grid, with Start (S) and Finish (F). The agent can choose from four possible actions in each state grid square: up , right , left, or down. The transitions between states are deterministic, meaning the robot will carry out the selected action with probability one.\n",
        "If the direction of movement is blocked, the agent remains in the same grid square i.e. if the robot is positioned at the edge of the grid and attempts to take an action that would move it outside the grid boundaries, it will remain in its current location. Upon reaching state F, the robot may earn a terminal reward of your choice, or you may also choose to set the state value function to a constant for any policy, which signifies the successful completion of its task. In all other states, the robot does not receive any rewards.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7ZfuqrSvSnqJ"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from operator import ne\n",
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.colors as mcolors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s84XOGQIS9um"
      },
      "source": [
        "Define a custom Grid World environment for Reinforcement Learning (RL) using OpenAI Gym."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VQpspUXX0Fno"
      },
      "outputs": [],
      "source": [
        "class GridWorldEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom 2D Grid World environment in Gym.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, grid_size=(5, 5), start=(0, 0), goal=(4, 4), gamma = 0.9):\n",
        "        super(GridWorldEnv, self).__init__()\n",
        "\n",
        "        # Define the grid world size\n",
        "        self.grid_size = grid_size\n",
        "\n",
        "        # Define start and goal positions\n",
        "        self.start = start\n",
        "        self.goal = goal\n",
        "\n",
        "        # Set the current position to the start\n",
        "        self.agent_pos = np.array(self.start)\n",
        "\n",
        "        # Define action space: 4 possible actions (up, down, left, right)\n",
        "        self.action_space = spaces.Discrete(4)  # 0 = down, 1 = left, 2 = up, 3 = right\n",
        "\n",
        "        # Define observation space (agent's position in the grid)\n",
        "        self.observation_space = spaces.Box(low=0, high=max(grid_size), shape=(2,), dtype=np.int32)\n",
        "\n",
        "        # State-value function initialized to 0\n",
        "        self.V = np.zeros(self.grid_size)\n",
        "\n",
        "        # Initialize the policy (a random policy for now, can be updated later)\n",
        "        # Policy: a 5x5 grid where each entry is one of the actions (0=down, 1=left, 2=up, 3=right)\n",
        "        self.policy = np.random.choice([0, 1, 2, 3], size=self.grid_size)\n",
        "\n",
        "        # Define\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Define the movement directions corresponding to actions\n",
        "        self.movement = {\n",
        "            0: np.array([1, 0]),  # down\n",
        "            1: np.array([0, -1]),  # left\n",
        "            2: np.array([-1, 0]),  # up\n",
        "            3: np.array([0, 1])  # right\n",
        "        }\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the environment to the initial state (start position).\n",
        "        \"\"\"\n",
        "        self.agent_pos = np.array(self.start)\n",
        "        return self.agent_pos\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Perform an action and update the environment's state.\n",
        "        \"\"\"\n",
        "        # Update the agent's position\n",
        "        next_pos = self.agent_pos + self.movement[action]\n",
        "\n",
        "        # Check if the new position is within grid bounds\n",
        "        if self._is_valid(next_pos):\n",
        "            self.agent_pos = next_pos\n",
        "\n",
        "        # Check if the agent has reached the goal\n",
        "        if tuple(self.agent_pos) == self.goal:\n",
        "            reward = 2*max(self.grid_size[1],self.grid_size[0])  # Large positive reward for reaching the goal\n",
        "            # reward = -1 # Treat the goal as any other state (In this case, one should set the state value function to a constant)\n",
        "            done = True\n",
        "        else:\n",
        "            reward = -1  # Small step penalty for each move\n",
        "            done = False\n",
        "\n",
        "        return self.agent_pos, reward, done, {}\n",
        "\n",
        "    def _is_valid(self, pos):\n",
        "        \"\"\"\n",
        "        Check if the new position is valid (within bounds).\n",
        "        \"\"\"\n",
        "        # Check if position is within bounds\n",
        "        if 0 <= pos[0] < self.grid_size[0] and 0 <= pos[1] < self.grid_size[1]:\n",
        "            return True\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WF838fYTwz0"
      },
      "source": [
        "Define plotting functions for the policy and the state value function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "e1VcUmALgyQV"
      },
      "outputs": [],
      "source": [
        "def plot_grid_with_policy(env):\n",
        "    \"\"\"This method plots the environment of the agent and shows the policy arrows.\n",
        "    Works for grid environments.\n",
        "\n",
        "    Args:\n",
        "        env (GridWorldEnv): The environment to be plotted.\n",
        "    \"\"\"\n",
        "    # Create a copy of the environment grid\n",
        "    grid = np.zeros(env.grid_size)\n",
        "\n",
        "    # Define custom colors for the colormap\n",
        "    colors = ['white', 'black']  # 'white' for free cells\n",
        "    cmap = mcolors.ListedColormap(colors)\n",
        "\n",
        "    # Plot the grid with the custom colormap\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.matshow(grid, cmap=cmap)\n",
        "\n",
        "    # Display 'F' for the finish line\n",
        "    ax.text(env.goal[1], env.goal[0], 'F', ha='center', va='center', color='black', fontsize=20)\n",
        "\n",
        "    # Define arrow directions based on policy actions\n",
        "    action_arrows = {\n",
        "        0: '↓',  # down\n",
        "        1: '←',  # left\n",
        "        2: '↑',  # up\n",
        "        3: '→'   # right\n",
        "    }\n",
        "\n",
        "    # Place arrows on the grid based on the policy\n",
        "    for i in range(env.grid_size[0]):\n",
        "        for j in range(env.grid_size[1]):\n",
        "            if (i, j) == env.goal:\n",
        "                continue\n",
        "            action = env.policy[i, j]  # Get action for the current state\n",
        "            ax.text(j, i, action_arrows[action], ha='center', va='center', color='blue', fontsize=20)\n",
        "\n",
        "    # Add grid lines separating each square\n",
        "    ax.set_xticks(np.arange(-0.5, env.grid_size[1], 1), minor=True)\n",
        "    ax.set_yticks(np.arange(-0.5, env.grid_size[0], 1), minor=True)\n",
        "    ax.grid(which='minor', color='black', linestyle='-', linewidth=1)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "    plt.title(\"Grid World with Policy Arrows\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_value_function(env):\n",
        "    \"\"\"Plot the value function with respect to an end point, including grid lines.\"\"\"\n",
        "    grid = env.V.copy()\n",
        "\n",
        "    # Create a new figure\n",
        "    plt.figure()\n",
        "\n",
        "    # Plot the grid representing the value function\n",
        "    plt.imshow(grid, cmap='Purples', interpolation='nearest', aspect='auto')\n",
        "    plt.colorbar()\n",
        "    plt.title(\"Agent's value function\")\n",
        "\n",
        "    # Add grid lines to the plot\n",
        "    ax = plt.gca()  # Get the current axis\n",
        "    ax.set_xticks(np.arange(-0.5, env.grid_size[1], 1), minor=True)\n",
        "    ax.set_yticks(np.arange(-0.5, env.grid_size[0], 1), minor=True)\n",
        "    ax.grid(which='minor', color='black', linestyle='-', linewidth=2)  # Add the grid lines\n",
        "\n",
        "    # Hide the tick labels (optional, for a cleaner plot)\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "\n",
        "    return plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nLDRFpX22zs0"
      },
      "outputs": [],
      "source": [
        "# Initialize the environment\n",
        "env = GridWorldEnv(grid_size=(5, 5), start=(0, 0), goal=(4, 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgZUU3sijhQ1"
      },
      "outputs": [],
      "source": [
        "# Plotting a random policy\n",
        "plot_grid_with_policy(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Theory Recap\n",
        "We would like to find a the optimal policy\n",
        "\n",
        "In order to apply Value Iteration or Policy Iteration, it is **necessary to have**:\n",
        "Model dynamics, specifically:\n",
        " * The **transition probabilities** $P(s' \\mid s, a)$, which describe the probability of transitioning from state $s$ to state $s'$ after taking action $a$.\n",
        " * The **reward function** $r(s, a)$, which specifies the reward received for taking action $a$ in state $s$.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Recall that the **state value function** for $s\\in \\mathcal{S}$ denoted\n",
        "\n",
        "$$V^\\pi(s)=\\sum_{a \\in A} \\pi(a \\mid s)\\left[r(s, a)+\\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) \\gamma V\\left(s^{\\prime}\\right)\\right]$$\n",
        "\n",
        "\n",
        "Since the **transition probabilities** are unknown we approximate the state value function with \n",
        "\n",
        "$$ V(S_t) \\leftarrow (1 - \\alpha)V(S_t) + \\alpha \\left( R_{t+1} + \\gamma V(S_{t+1}) \\right) $$\n",
        "\n",
        "In theory we should visit each state infinitely many times, but in practical cases the algorithm usually terminates after some criterion ( i.e. small TD error over time, Decay in learning rate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EfIDBuaGi_97"
      },
      "outputs": [],
      "source": [
        "def policy_improvement(env, num_episodes=100):\n",
        "    \"\"\"\n",
        "    Perform policy improvement for the GridWorld environment based on the learned value function (V).\n",
        "\n",
        "    This function updates the policy by evaluating all possible actions at each state and selecting\n",
        "    the action that leads to the highest expected future reward. It performs a type of greedy action\n",
        "    selection based on the value function estimates.\n",
        "    \"\"\"\n",
        "\n",
        "    for i in range(env.grid_size[0]):\n",
        "        for j in range(env.grid_size[1]):\n",
        "            state = np.array([i, j])\n",
        "            # Skip the goal state since no action is required there\n",
        "            if tuple(state) == env.goal:\n",
        "                continue\n",
        "            # Variables to track the best action and its value for each state\n",
        "            best_action = None\n",
        "            best_value = -float('inf')\n",
        "\n",
        "            # Loop through all possible actions (down, left, up, right)\n",
        "            for action in range(env.action_space.n):\n",
        "                reward_action = 0  # Initialize reward for the current action\n",
        "\n",
        "                # Run multiple episodes to estimate the value of taking this action\n",
        "                for k in range(num_episodes):\n",
        "                    state = np.array([i, j])  # Reset state to the current (i, j)\n",
        "                    next_state = state + env.movement[action]  # Calculate the resulting state from the action\n",
        "\n",
        "                    # Check if the resulting state is valid\n",
        "                    if env._is_valid(next_state):\n",
        "                        next_state_tuple = tuple(next_state)\n",
        "                        # Accumulate the value of the next state (from V) as part of the reward\n",
        "                        reward_action += env.V[next_state_tuple]\n",
        "                    else:\n",
        "                        # Penalize for invalid moves (out of bounds)\n",
        "                        reward_action -= 10\n",
        "\n",
        "                # Average the reward over the episodes for more reliable action evaluation\n",
        "                reward_action /= num_episodes\n",
        "\n",
        "                # Keep track of the best action for the current state\n",
        "                if reward_action > best_value:\n",
        "                    best_value = reward_action\n",
        "                    best_action = action\n",
        "\n",
        "            # Update the policy at (i, j) to take the best action found\n",
        "            env.policy[i, j] = best_action\n",
        "\n",
        "\n",
        "def td_learning(env, epsilon=0.5):\n",
        "    \"\"\"\n",
        "    Perform TD(0) learning.\n",
        "    \"\"\"\n",
        "    visit_count = np.zeros(env.grid_size)\n",
        "    # In case of no reward for reaching the goal\n",
        "    # env.V[tuple(env.goal)] = max(env.grid_size[1], env.grid_size[0])\n",
        "\n",
        "    while True:\n",
        "        compare = env.V.copy()\n",
        "\n",
        "        state = env.reset()  # Start from the beginning\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            state = env.agent_pos\n",
        "\n",
        "            # Epsilon-greedy action selection\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = env.action_space.sample()  # Explore\n",
        "            else:\n",
        "                action = env.policy[tuple(state)]  # Exploit\n",
        "\n",
        "            # Take the action\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Convert state and next_state from arrays to tuple for indexing\n",
        "            state_tuple = tuple(state)\n",
        "            next_state_tuple = tuple(next_state)\n",
        "\n",
        "            # Increment visit count for the current state\n",
        "            visit_count[next_state_tuple] += 1\n",
        "\n",
        "            # Calculate learning rate for the current state\n",
        "            alpha_s = 1 / visit_count[next_state_tuple]\n",
        "\n",
        "            # TD update\n",
        "            env.V[state_tuple] = env.V[state_tuple] + alpha_s * (\n",
        "                reward + env.gamma * env.V[next_state_tuple] - env.V[state_tuple]\n",
        "            )\n",
        "\n",
        "            state = next_state  # Update the state variable\n",
        "\n",
        "        delta = np.max(np.abs(compare - env.V))\n",
        "\n",
        "        if delta < 0.0001:\n",
        "            break\n",
        "\n",
        "def optimal_policy(env):\n",
        "    \"\"\"\n",
        "    Find the optimal policy using temporal difference learning.\n",
        "    \"\"\"\n",
        "    # Policy iteration\n",
        "    while True:\n",
        "        new_policy = env.policy.copy()\n",
        "\n",
        "        # Policy evaluation via td - learning\n",
        "        td_learning(env)\n",
        "\n",
        "        # # Policy improvement\n",
        "        policy_improvement(env)\n",
        "\n",
        "        # Break if there is no improvement\n",
        "        if np.array_equal(env.policy, new_policy):\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4b3oz9mZAwa-"
      },
      "outputs": [],
      "source": [
        "# Calculate the optimal policy\n",
        "optimal_policy(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrmJvtMWBD1-"
      },
      "outputs": [],
      "source": [
        "# Plot the state action value function for the optimal policy\n",
        "plot = plot_value_function(env)\n",
        "plot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Mbq_EnCBYxE"
      },
      "outputs": [],
      "source": [
        "# Plot the optimal policy\n",
        "plot_grid_with_policy(env)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
